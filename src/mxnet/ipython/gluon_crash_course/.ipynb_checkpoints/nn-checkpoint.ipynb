{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a neural network\n",
    "\n",
    "Now let's look how to create neural networks in Gluon. In addition the NDArray package (`nd`) that we just covered, we now will also import the neural network `nn` package from `gluon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your neural network's first layer\n",
    "\n",
    "Let's start with a dense layer with 2 output units.\n",
    "<!-- mention what the none and the linear parts mean? -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "31"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(None -> 2, linear)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Dense(2)\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then initialize its weights with the default initialization method, which draws random values uniformly from $[-0.7, 0.7]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "32"
    }
   },
   "outputs": [],
   "source": [
    "layer.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do a forward pass with random data. We create a $(3,4)$ shape random input `x` and feed into the layer to compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "34"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.02524132 -0.00874885]\n",
       " [-0.06026538 -0.01308061]\n",
       " [ 0.02468396 -0.02181557]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.random.uniform(-1,1,(3,4))\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the layer's input limit of 2 produced a $(3,2)$ shape output from our $(3,4)$ input. Note that we didn't specify the input size of `layer` before (though we can specify it with the argument `in_units=4` here), the system will automatically infer it during the first time we feed in data, create and initialize the weights. So we can access the weight after the first forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "35"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.00873779 -0.02834515  0.05484822 -0.06206018]\n",
       " [ 0.06491279 -0.03182812 -0.01631819 -0.00312688]]\n",
       "<NDArray 2x4 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain layers into a neural network\n",
    "\n",
    "Let's first consider a simple case that a neural network is a chain of layers. During the forward pass, we run layers sequentially one-by-one. The following code implements a famous network called [LeNet](http://yann.lecun.com/exdb/lenet/) through `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n",
       "  (2): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n",
       "  (4): Flatten\n",
       "  (5): Dense(None -> 120, Activation(relu))\n",
       "  (6): Dense(None -> 84, Activation(relu))\n",
       "  (7): Dense(None -> 10, linear)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "# Add a sequence of layers.\n",
    "net.add(\n",
    "    # Similar to Dense, it is not necessary to specify the\n",
    "    # input channels by the argument `in_channels`, which will be\n",
    "    # automatically inferred in the first forward pass. Also,\n",
    "    # we apply a relu activation on the output.\n",
    "    #\n",
    "    # In addition, we can use a tuple to specify a\n",
    "    # non-square kernel size, such as `kernel_size=(2,4)`\n",
    "    nn.Conv2D(channels=6, kernel_size=5, activation='relu'),\n",
    "    # One can also use a tuple to specify non-symmetric\n",
    "    # pool and stride sizes\n",
    "    nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    nn.Conv2D(channels=16, kernel_size=3, activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    # flatten the 4-D input into 2-D with shape\n",
    "    # `(x.shape[0], x.size/x.shape[0])` so that it can be used\n",
    "    # by the following dense layers\n",
    "    nn.Flatten(),\n",
    "    nn.Dense(120, activation=\"relu\"),\n",
    "    nn.Dense(84, activation=\"relu\"),\n",
    "    nn.Dense(10))\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Mention the tuple option for kernel and stride as an exercise for the reader? Or leave it out as too much info for now?-->\n",
    "\n",
    "The usage of `nn.Sequential` is similar to `nn.Dense`. In fact, both of them are subclasses of `nn.Block`. The following codes show how to initialize the weights and run the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "# Input shape is (batch_size, color_channels, height, width)\n",
    "x = nd.random.uniform(shape=(4,1,28,28))\n",
    "y = net(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `[]` to index a particular layer. For example, the following\n",
    "accesses the 1st layer's weight and 6th layer's bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 1, 5, 5), (120,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net[0].weight.data().shape, net[5].bias.data().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a neural network flexibly\n",
    "\n",
    "In `nn.Sequential`, MXNet will automatically construct the forward function that sequentially executes added layers.\n",
    "Now let's introduce another way to construct a network with a flexible forward function.\n",
    "\n",
    "To do it, we create a subclass of `nn.Block` and implement two methods:\n",
    "\n",
    "- `__init__` create the layers\n",
    "- `forward` define the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixMLP(\n",
       "  (blk): Sequential(\n",
       "    (0): Dense(None -> 3, Activation(relu))\n",
       "    (1): Dense(None -> 4, Activation(relu))\n",
       "  )\n",
       "  (dense): Dense(None -> 5, linear)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MixMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Run `nn.Block`'s init method\n",
    "        super(MixMLP, self).__init__(**kwargs)\n",
    "        self.blk = nn.Sequential()\n",
    "        self.blk.add(nn.Dense(3, activation='relu'),\n",
    "                     nn.Dense(4, activation='relu'))\n",
    "        self.dense = nn.Dense(5)\n",
    "    def forward(self, x):\n",
    "        y = nd.relu(self.blk(x))\n",
    "        print(y)\n",
    "        return self.dense(y)\n",
    "\n",
    "net = MixMLP()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sequential chaining approach, we can only add instances with `nn.Block` as the base class and then run them in a forward pass. In this example, we used `print` to get the intermediate results and `nd.relu` to apply relu activation. So this approach provides a more flexible way to define the forward function.\n",
    "\n",
    "The usage of `net` is similar as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[  0.00000000e+00   0.00000000e+00   6.29003858e-04   7.64455399e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   1.19893858e-03   1.23752037e-03]]\n",
      "<NDArray 2x4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ -3.80618403e-05   1.55683501e-05   4.36682149e-06   4.28530584e-05\n",
       "    1.87103942e-05]\n",
       " [ -1.83455195e-05   2.64030787e-05   2.46857308e-05   7.70193728e-05\n",
       "    9.77859891e-05]]\n",
       "<NDArray 2x5 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "x = nd.random.uniform(shape=(2,2))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's access a particular layer's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.0343901  -0.05805862 -0.06187592]\n",
       " [-0.06210143 -0.00918167 -0.00170272]\n",
       " [-0.02634858  0.05334064  0.02748809]\n",
       " [ 0.06669661 -0.01711474  0.01647211]]\n",
       "<NDArray 4x3 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.blk[1].weight.data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
