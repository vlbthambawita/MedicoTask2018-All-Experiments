{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfering knowledge through finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo mode uses the validation dataset for training, which is smaller and faster to train.\n",
    "demo = True\n",
    "log_interval = 100\n",
    "gpus = 0\n",
    "\n",
    "# Options are imperative or hybrid. Use hybrid for better performance.\n",
    "mode = 'hybrid'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 256\n",
    "if demo:\n",
    "    epochs = 5\n",
    "    learning_rate = 0.02\n",
    "    wd = 0.002\n",
    "else:\n",
    "    epochs = 40\n",
    "    learning_rate = 0.05\n",
    "    wd = 0.002\n",
    "\n",
    "# the class weight for hotdog class to help the imbalance problem.\n",
    "positive_class_weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import skimage.io as io\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.test_utils import download\n",
    "mx.random.seed(127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My data folder path\n",
    "data_dir = '../../../data/data_hot_dogs'\n",
    "train_dir = data_dir + '/train'\n",
    "validation_dir = data_dir + '/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_generated_medicotask_v1',\n",
       " 'data_v2',\n",
       " 'data_hot_dogs',\n",
       " 'my_models',\n",
       " 'data_old',\n",
       " 'pytorch_models',\n",
       " 'data_Medico_2018_development_set_v1',\n",
       " 'data_Medico_2018_development_set_v2',\n",
       " 'history_of_training',\n",
       " 'plots',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "    \n",
    "if not os.path.exists(validation_dir):\n",
    "    os.mkdir(validation_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = {'train': ('not_hotdog_train-e6ef27b4.rec', '0aad7e1f16f5fb109b719a414a867bbee6ef27b4'),\n",
    "                 'validation': ('not_hotdog_validation-c0201740.rec', '723ae5f8a433ed2e2bf729baec6b878ac0201740')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo:\n",
    "    training_dataset, training_data_hash = dataset_files['validation']\n",
    "else:\n",
    "    training_dataset, training_data_hash = dataset_files['train']\n",
    "\n",
    "validation_dataset, validation_data_hash = dataset_files['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verified(file_path, sha1hash):\n",
    "    import hashlib\n",
    "    sha1 = hashlib.sha1()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while True:\n",
    "            data = f.read(1048576)\n",
    "            if not data:\n",
    "                break\n",
    "            sha1.update(data)\n",
    "    matched = sha1.hexdigest() == sha1hash\n",
    "    if not matched:\n",
    "        logging.warn('Found hash mismatch in file {}, possibly due to incomplete download.'\n",
    "                     .format(file_path))\n",
    "    return matched\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading training dataset.\n",
      "INFO:root:downloaded https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/not_hotdog_validation-c0201740.rec into ../../../data/data_hot_dogs/train/not_hotdog_validation-c0201740.rec successfully\n"
     ]
    }
   ],
   "source": [
    "url_format = 'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/{}'\n",
    "if not os.path.exists(os.path.join(train_dir,training_dataset)) or not verified(os.path.join(train_dir,training_dataset), training_data_hash):\n",
    "    logging.info('Downloading training dataset.')\n",
    "    download(url_format.format(training_dataset),\n",
    "             overwrite=True, dirname=train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading validation dataset.\n",
      "INFO:root:downloaded https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/not_hotdog_validation-c0201740.rec into ../../../data/data_hot_dogs/validation/not_hotdog_validation-c0201740.rec successfully\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(validation_dir,validation_dataset)) or not verified(os.path.join(validation_dir,validation_dataset), validation_data_hash):\n",
    "    logging.info('Downloading validation dataset.')\n",
    "    download(url_format.format(validation_dataset),\n",
    "             overwrite=True, dirname=validation_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
